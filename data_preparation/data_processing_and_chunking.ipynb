{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing and Chunking\n",
        "\n",
        "In this notebook we are going to:\n",
        "\n",
        "\n",
        "1.   Read the document using pypdf\n",
        "2.   Use pypdf to remove the headers and footers containing unnessary information\n",
        "3.   Remove repetitive and non-informative sentences in the page texts using str.replace()\n",
        "4.   Chunk by sentences using NLTK\n",
        "5.   Remove non-paragraphic parts using a rule - every sentence should have at least 2 stop words - done using NLTK\n",
        "6.   Create two CSV files - one containing the information of a whole page and one chunking the information by context, both are featuring unique keys that can be used to retrieve the original source.\n",
        "\n"
      ],
      "metadata": {
        "id": "9QaPfHmnxd3d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjtSpjdOd-iP",
        "outputId": "e6151bd7-7d80-4d62-bc8e-253e34b0a6b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "file_location = '/content/drive/My Drive/Colab Notebooks/Interview_tasks/JSNOW/Data'"
      ],
      "metadata": {
        "id": "cdz6e8hgeLCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopWords = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "foyW2HhNeUJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def remove_non_ascii(string):\n",
        "    return \"\".join(\n",
        "        char\n",
        "        for char in string\n",
        "        if ord(char) < 128\n",
        "    )\n",
        "\n",
        "\n",
        "def get_splits(text, n_sent=3):\n",
        "    tokens = 0\n",
        "    subset = []\n",
        "    total = []\n",
        "    for sent in text:\n",
        "        tokens += len(sent.split())\n",
        "        if tokens <= 250:\n",
        "            subset.append(sent)\n",
        "        else:\n",
        "            total.append(subset)\n",
        "            if n_sent == 0:\n",
        "                subset = []\n",
        "            else:\n",
        "                subset = subset[-n_sent:]\n",
        "            tokens = 0\n",
        "            for sub in subset:\n",
        "                tokens += len(sub.split())\n",
        "            subset.append(sent)\n",
        "            tokens += len(sent.split())\n",
        "    total.append(subset)\n",
        "    return total"
      ],
      "metadata": {
        "id": "4wfXL4HneUpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "import os\n",
        "\n",
        "reader = PdfReader(os.path.join(file_location, 'test_guideline.pdf'))\n",
        "page = reader.pages[70]\n",
        "\n",
        "\n",
        "def read_a_page(page):\n",
        "  parts = []\n",
        "\n",
        "  def visitor_body(text, cm, tm, font_dict, font_size):\n",
        "      y = tm[5]\n",
        "      if 50 < y < 720:\n",
        "          parts.append(text)\n",
        "  page.extract_text(visitor_text=visitor_body)\n",
        "  text_body = \"\".join(parts)\n",
        "  text_body = text_body.replace(\"Downloaded from http://ahajournals.org by on March 16, 2024\", \"\")\n",
        "  text_body = text_body.replace(\"CLINICAL STATEMENTS AND GUIDELINES\", \"\")\n",
        "  return text_body\n",
        "\n",
        "def get_page_splits(page_text):\n",
        "  text_sent = sent_tokenize(page_text)\n",
        "  text_sent = [str(sent).replace(\"\\r\\n\", \" \") for sent in text_sent\n",
        "              #Removal of sentences without at least 2 stopwords\n",
        "              if len(set(sent.split()).intersection(stopWords))>1]\n",
        "  text_sentence_splits = get_splits(text_sent)\n",
        "  return text_sentence_splits\n",
        "\n",
        "def create_page_index(text_sentence_splits, page_num):\n",
        "  page_data = []\n",
        "  for idx, split in enumerate(text_sentence_splits):\n",
        "    page_context_row = {\n",
        "        \"context_uid\":str(page_num)+\"_\"+str(idx),\n",
        "        \"page\": page_num,\n",
        "        \"context_index\": idx,\n",
        "        \"context\": \" \".join(split),\n",
        "    }\n",
        "    page_data.append(page_context_row)\n",
        "\n",
        "  return page_data\n",
        "\n",
        "df_list = []\n",
        "df_page_clean = []\n",
        "for index, page in enumerate(reader.pages):\n",
        "  if index<6 or index>108:\n",
        "    continue\n",
        "  page_text = read_a_page(page)\n",
        "  page_splits = get_page_splits(page_text)\n",
        "  page_data = create_page_index(page_splits, index+1)\n",
        "  df_list.extend(page_data)\n",
        "  df_page_clean.append({\"page_text\":page_text, \"page\":index+1})\n",
        "\n",
        "df_page_info = pd.DataFrame(df_page_clean)\n",
        "df_final = pd.DataFrame(df_list)\n",
        "df_final.head()"
      ],
      "metadata": {
        "id": "6LnKN0QVeXKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_page_info.to_csv(os.path.join(file_location, \"full_page_info.csv\"), index=False)"
      ],
      "metadata": {
        "id": "fHJV0SsoebFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.to_csv(os.path.join(file_location, \"data_cleaned.csv\"), index=False)"
      ],
      "metadata": {
        "id": "poVMeOyheeGE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}