{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVU_ul-v-t5Y"
      },
      "source": [
        "# API notebook\n",
        "Here we are going to deploy the 4 endpoints of the embeddings, reranker and generative model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install necessary dependencies:\n",
        "\n",
        "\n",
        "*   pyngrok - for tunneling the API endpoints\n",
        "*   vllm - for productionalizing the large language models\n",
        "*   uvicorn & fastapi - for creating the API\n",
        "* sentence-transformers - for performing sentence level semantic search using embedders and cross encoders\n",
        "\n"
      ],
      "metadata": {
        "id": "rWn31IubvQT1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3t0anldUfw2"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL70BMGwxJl6"
      },
      "outputs": [],
      "source": [
        "!pip install -q vllm fastapi uvicorn ngrok pyngrok nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhMo9TAAge7o"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzT3M3QKUKee"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.update()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add your ngrok token here:"
      ],
      "metadata": {
        "id": "beUnqrzewzQP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S_TCp7exG0G",
        "outputId": "63c3f3e5-65ff-4b18-acd1-705c70d9603c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "#!ngrok config add-authtoken <add token here>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-kv1NBnUmnu"
      },
      "outputs": [],
      "source": [
        "from pyngrok import conf, ngrok\n",
        "\n",
        "conf.get_default().ngrok_path = \"/usr/local/bin/ngrok\"\n",
        "\n",
        "# <NgrokTunnel: \"https://<public_sub>.ngrok.io\" -> \"http://localhost:80\">\n",
        "ngrok_tunnel = ngrok.connect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqFOccA5k6S6",
        "outputId": "f50c5434-611f-4e40-df16-87f141a48d31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9saVfAT4kuuv"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "def load_dataset_in_index(model, data):\n",
        "    \"\"\"\n",
        "    Loads a dataset into a FAISS index after encoding the data using a given model.\n",
        "\n",
        "    Args:\n",
        "        model (object): The model used to encode the data, expected to have an 'encode' method.\n",
        "        data (pandas.DataFrame): DataFrame containing the data to be indexed, should have a \"context\" column.\n",
        "\n",
        "    Returns:\n",
        "        faiss.Index: The FAISS index with the encoded data.\n",
        "    \"\"\"\n",
        "    df = data[[\"context\"]]\n",
        "    encoded_data = model.encode(df.context.tolist())\n",
        "    encoded_data = np.asarray(encoded_data.astype(\"float32\"))\n",
        "    index = faiss.IndexIDMap(faiss.IndexFlatIP(768))\n",
        "    index.add_with_ids(encoded_data, np.array(range(0, len(df))))\n",
        "    faiss.write_index(index, os.path.join(file_location, \"context.index\"))\n",
        "    return index\n",
        "\n",
        "def fetch_info(dataframe_idx, df):\n",
        "    \"\"\"\n",
        "    Fetches context and page information from a DataFrame given a row index.\n",
        "\n",
        "    Args:\n",
        "        dataframe_idx (int): Row index in the DataFrame.\n",
        "        df (pandas.DataFrame): DataFrame containing the data.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the \"context\" and \"page\" information.\n",
        "    \"\"\"\n",
        "    info = df.iloc[dataframe_idx]\n",
        "    meta_dict = {}\n",
        "    meta_dict[\"context\"] = info[\"context\"]\n",
        "    meta_dict[\"page\"] = info[\"page\"]\n",
        "    return meta_dict\n",
        "\n",
        "\n",
        "def search(query, top_k, index, model, df):\n",
        "    \"\"\"\n",
        "    Searches the FAISS index with a given query and returns the top K results.\n",
        "\n",
        "    Args:\n",
        "        query (str): Query string to search for.\n",
        "        top_k (int): Number of top results to return.\n",
        "        index (faiss.Index): The FAISS index to search in.\n",
        "        model (object): The model used to encode the query, expected to have an 'encode' method.\n",
        "        df (pandas.DataFrame): DataFrame containing the data.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries containing the \"context\" and \"page\" information of the top results.\n",
        "    \"\"\"\n",
        "    t = time.time()\n",
        "    query_vector = model.encode([query])\n",
        "    top_k = index.search(query_vector, top_k)\n",
        "    #print(top_k)\n",
        "    print(\">>>> Results in Total Time: {}\".format(time.time() - t))\n",
        "    top_k_ids = top_k[1].tolist()[0]\n",
        "    top_k_ids = list(np.unique(top_k_ids))\n",
        "    results = [fetch_info(idx, df) for idx in top_k_ids]\n",
        "    return results\n",
        "\n",
        "def rerank(model, query, embedding_results):\n",
        "    \"\"\"\n",
        "    Re-ranks the embedding results using a cross-encoder model.\n",
        "\n",
        "    Args:\n",
        "        model (object): The re-ranking model, expected to have a 'predict' method.\n",
        "        query (str): The query string.\n",
        "        embedding_results (list): List of embedding results to be re-ranked.\n",
        "\n",
        "    Returns:\n",
        "        str: The context of the highest-scoring result.\n",
        "    \"\"\"\n",
        "    model_inputs = [[query, item] for item in embedding_results]\n",
        "    scores = model.predict(model_inputs)\n",
        "    ranked_results = [\n",
        "        {\"Context\": inp, \"Score\": score}\n",
        "        for inp, score in zip(embedding_results, scores)\n",
        "    ]\n",
        "    ranked_results = sorted(ranked_results, key=lambda x: x[\"Score\"], reverse=True)\n",
        "    best_result = ranked_results[0][\"Context\"]\n",
        "    print(\"Best result score: \" + str(ranked_results[0][\"Score\"]))\n",
        "    return best_result\n",
        "\n",
        "\n",
        "\n",
        "def get_results(model, df, index, question):\n",
        "    \"\"\"\n",
        "    Gets the final re-ranked results for a given query.\n",
        "\n",
        "    Args:\n",
        "        model (object): The main model used for encoding and re-ranking.\n",
        "        df (pandas.DataFrame): DataFrame containing the data.\n",
        "        index (faiss.Index): The FAISS index to search in.\n",
        "        question (str): The query string.\n",
        "\n",
        "    Returns:\n",
        "        list: A list containing a dictionary with \"context\", \"page\", and \"context_uid\" of the best result.\n",
        "    \"\"\"\n",
        "    query = question\n",
        "    embedding_results = search(query, top_k=50, index=index, model=model, df=df)\n",
        "    embedding_results_ctx = [item[\"context\"] for item in embedding_results]\n",
        "    final_result = rerank(cross_encoder, query, embedding_results_ctx)\n",
        "    context_row = df[df[\"context\"]==final_result]\n",
        "    final_result_page = context_row.page.values[0]\n",
        "    final_context_uid = context_row.context_uid.values[0]\n",
        "    reranker_results = [{\"context\": final_result, \"page\":final_result_page, \"context_uid\":final_context_uid}]\n",
        "    return reranker_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a variable holding the generative model, so that it is not loaded every time we restart the application:"
      ],
      "metadata": {
        "id": "SI2p5v-Ywa0b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4sJ4HAPjk8I"
      },
      "outputs": [],
      "source": [
        "llm = None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following and then copy the ngrok funnel link to the client side application:"
      ],
      "metadata": {
        "id": "IPYMAEhvwhKf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aOgkYFojxL0W"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from vllm import LLM, SamplingParams\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "from starlette.responses import StreamingResponse\n",
        "import asyncio\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI()\n",
        "if llm is None:\n",
        "  llm = LLM(model=\"Sreenington/Phi-3-mini-4k-instruct-AWQ\", quantization=\"AWQ\")\n",
        "\n",
        "class CompletionRequest(BaseModel):\n",
        "    prompt: str\n",
        "    max_tokens: int\n",
        "    temperature: float\n",
        "\n",
        "class SearchRequest(BaseModel):\n",
        "  query: str\n",
        "\n",
        "class RerankRequest(BaseModel):\n",
        "  model_args: str\n",
        "\n",
        "@app.on_event('startup')\n",
        "def load_search_models():\n",
        "  global embedder\n",
        "  global cross_encoder\n",
        "  embedder = SentenceTransformer(\n",
        "          \"sentence-transformers/msmarco-bert-base-dot-v5\", device=\"cuda\"\n",
        "      )\n",
        "  cross_encoder = CrossEncoder(\n",
        "        \"cross-encoder/ms-marco-MiniLM-L-12-v2\", device=\"cuda\"\n",
        "    )\n",
        "\n",
        "@app.post(\"/v1/completions\")\n",
        "async def completions(request: CompletionRequest):\n",
        "  sampling_params = SamplingParams(max_tokens=request.max_tokens, temperature=request.temperature)\n",
        "  response = llm.generate([request.prompt], sampling_params)\n",
        "  ans = response[0].outputs[0].text\n",
        "  return ans\n",
        "\n",
        "@app.post(\"/v1/search\")\n",
        "def embed(request: SearchRequest):\n",
        "  global embedder\n",
        "  query_vector = embedder.encode([request.query])\n",
        "  return query_vector\n",
        "\n",
        "@app.post(\"/v1/rerank\")\n",
        "async def rerank(request: RerankRequest):\n",
        "  global cross_encoder\n",
        "  prediction = cross_encoder.predict(request.model_args)\n",
        "\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohxxLIR_ZSEo"
      },
      "outputs": [],
      "source": [
        "#del llm\n",
        "del embedder\n",
        "del cross_encoder"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}